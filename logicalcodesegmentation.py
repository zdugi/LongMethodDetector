# -*- coding: utf-8 -*-
"""LogicalCodeSegmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5klEmeB7cC8cOGobC2kue8pNxBS3-Ri
"""

import pandas as pd
import numpy as np

! pip install -q kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

!kaggle datasets download -d zdravkodugonjic/code-snippets

!unzip \*.zip  && rm *.zip

code_snippets = pd.read_csv('code_snippets.csv')

from html.parser import HTMLParser
import html

start_tag = '<code>'
end_tag = '</code>'

parser = HTMLParser()
raw_snippet = code_snippets.at[0, 'unnested_code_snippets'][len(start_tag):-len(end_tag)]
print(raw_snippet)

decoded = html.unescape(raw_snippet)
print(decoded)

snippets_list = code_snippets['unnested_code_snippets'].to_numpy()

def force_ascii(string):
    encoded_string = string.encode('ascii', 'ignore')
    return encoded_string.decode()

clean_snippets = [force_ascii(html.unescape(raw_snippet[len(start_tag):-len(end_tag)])) for raw_snippet in snippets_list]

giant_block = '\n'.join(clean_snippets)

segment_new_lines = [len(clean_snippets[0])]
for i in range(1, len(clean_snippets)):
    segment_new_lines.append(segment_new_lines[i - 1] +  len(clean_snippets[i]) + 1)

giant_block[segment_new_lines[123]]

import pickle
from tqdm import tqdm
import gc
import os

CHUNK_CAPACITY = 2**7

''' return (window, labels) '''
def datapoints(code_block, window_size, dividing_newlines):
    i = 0
    label_container = []
    window_container = []
    chunk = 0
    total = 0
    
    if not os.path.exists('tmp'):
        os.mkdir('tmp')
    
    for window_start in tqdm(range(0, len(code_block), window_size)):
        labels = []
        window = []
        
        for ch_index in range(window_start, min(window_start+window_size, len(code_block))):
            if dividing_newlines[i] != ch_index:
                labels.append(0)
            else:
                labels.append(1)
                i += 1
            window.append(ord(code_block[ch_index]))
        
        label_container.append(np.array(labels, dtype='int'))
        window_container.append(np.array(window, dtype='float64'))
        total += 1
        
        if len(window_container) == CHUNK_CAPACITY:
            chunkd = {'windows': window_container, 'labels': label_container}
            with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:
                pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)
            chunk += 1
            label_container = []
            window_container = []
    
    # cleaning
    if len(window_container) > 0:
        chunkd = {'windows': window_container, 'labels': label_container}
        with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:
            pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)
        chunk += 1
        del chunkd
        label_container = []
        window_container = []
                
        #window_np = [ord(ch) for ch in code_block[window_start:window_start+window_size]]
        #labels_np = labels
        
        #yield (window_np, labels_np)
        #yield labels
    
    return total, chunk

total, chunks = datapoints(giant_block, 100, segment_new_lines)

import time

watch_point = time.time()

with open('tmp/x_chunk_0.pickle', 'rb') as handle:
    b = pickle.load(handle)

print('[' + str(CHUNK_CAPACITY) + '] load time: ' + str(time.time() - watch_point))

# try without last
total_new = int(total / CHUNK_CAPACITY) * CHUNK_CAPACITY
chunks_new = int(total / CHUNK_CAPACITY)
print('Preserved samples: ' + str(total_new / total))

from keras.utils import Sequence
import random

class GDataGenerator(Sequence):
    def __init__(self, chunks_names, batch_size):
        self.total_chunks = len(chunks_names)
        self.total_samples = self.total_chunks * CHUNK_CAPACITY
        self.batch_size = batch_size
        self.chunks_names = chunks_names
        
        #self.chunk_indices = list(range(self.total_chunks)) # take care of last
        
        #self.loaded_chunks = 0
        #self.loaded_chunk = None
        
        self.on_epoch_end()
        
    # FOR NOW PROVIDE FULL
    def __len__(self):
        return int(self.total_samples / self.batch_size)
    
    def __getitem__(self, index):
        if not (index * self.batch_size < self.loaded_chunks * CHUNK_CAPACITY):
            with open('tmp/x_chunk_%d.pickle' % (self.chunks_names[self.loaded_chunks]), 'rb') as handle:
                self.loaded_chunk = pickle.load(handle)
                
                # more shuffle
                #c = list(zip(self.loaded_chunk['windows'], self.loaded_chunk['labels']))
                #random.shuffle(c)
                #self.loaded_chunk['windows'], self.loaded_chunk['labels'] = zip(*c)
                
                self.loaded_chunks += 1
        
        X = np.array(self.loaded_chunk['windows'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='float64')
        y = np.array(self.loaded_chunk['labels'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='int')
        
        return X, y
        
    def on_epoch_end(self):
        # reset pointer to begin
        self.loaded_chunks = 0

        #self.chunk_indices = list(range(self.total_chunks))
        #random.shuffle(self.chunk_indices)
        random.shuffle(self.chunks_names)
        #self.chunk_indices.append(self.total_chunks - 1) # because last is not full

print('Percent of positive: ' + str(100 * len(segment_new_lines) / len(giant_block)))

# gen
all_chunks = list(range(chunks_new))

gtrain_lim = int(len(all_chunks) * .8)
gvalid_lim = gtrain_lim + int(len(all_chunks) * .1)

gtrain = all_chunks[:gtrain_lim]
gvalid = all_chunks[gtrain_lim:gvalid_lim]
gtest = all_chunks[gvalid_lim:]
len(gtrain)/len(all_chunks), len(gtrain), len(gvalid)/len(all_chunks), len(gvalid), len(gtest) / len(all_chunks), len(gtest)

gtraining_generator = GDataGenerator(gtrain, 128)
gvalidation_generator = GDataGenerator(gvalid, 128)

from keras.models import Sequential, load_model
from keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense, TimeDistributed, Flatten
from keras.metrics import AUC, Precision, Recall, TruePositives, TrueNegatives, BinaryAccuracy

n_vocab = 256 
num_lstm_units = 256 
bimodel = Sequential() 
bimodel.add(Embedding(n_vocab,20, input_length =100)) 
bimodel.add(Dropout(.2)) 
bimodel.add(Bidirectional(LSTM(num_lstm_units, return_sequences=True))) 
bimodel.add(Dropout(.2)) 
bimodel.add(TimeDistributed(Dense(150, activation='relu'))) 
bimodel.add(Dropout(.2)) 
bimodel.add(TimeDistributed(Dense(75, activation = 'relu'))) 
bimodel.add(Dropout(.2)) 
bimodel.add(TimeDistributed(Dense(1, activation = 'sigmoid'))) 
bimodel.add(Flatten())
# TAKE CARE
#bimodel = load_model('/content/drive/MyDrive/data/model-1-0.1974201202392578.h5')

bimodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['acc', BinaryAccuracy(), AUC(curve='PR'), Precision(), Recall()])

from keras.callbacks import ModelCheckpoint, EarlyStopping

checkpoint_filepath = '/content/drive/MyDrive/data/model-{epoch}-{acc}.h5'
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='acc',
    save_freq='epoch')
'''monitor='loss',
    mode='min',
    save_best_only=True'''

model = bimodel.fit_generator(
    generator=gtraining_generator,
    validation_data=gvalidation_generator,
    callbacks=[model_checkpoint_callback, EarlyStopping(monitor='loss', patience=15)],
    epochs=2000)

