{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import bq_helper\nfrom bq_helper import BigQueryHelper\n\nstackOverflow = BigQueryHelper(active_project='bigquery-public-data', dataset_name='stackoverflow')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"stackoverflow\")\nbq_assistant.list_tables()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#query = 'SELECT x.id, x.body FROM bigquery-public-data.stackoverflow.posts_questions as x WHERE x.tags LIKE \"%java%\" OR x.tags LIKE \"%android%\" AND x.body LIKE \"%<code>%\"'\n#query = 'SELECT COUNT(x) FROM bigquery-public-data.stackoverflow.posts_answers as x WHERE x.body LIKE \"%<code>%\"'\n#response = stackOverflow.query_to_pandas_safe(query, max_gb_scanned=30)\n#response\n\n#query = 'SELECT ide, REGEXP_EXTRACT_ALL(body, r\\'(<code>[\\s\\S]*?<\\/code>)\\') as code FROM (SELECT x.id as ide, x.body as body FROM bigquery-public-data.stackoverflow.posts_questions as x WHERE x.tags LIKE \"%java%\" OR x.tags LIKE \"%android%\" AND x.body LIKE \"%<code>%\")'\n#response2 = stackOverflow.query_to_pandas_safe(query, max_gb_scanned=30)\n#response2\n\n#query3 = 'SELECT answer_id, LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\")) FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" OR question.tags LIKE \"%android%\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets'\nquery3 = 'SELECT answer_id, LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\")) FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets'\n\nresponse3 = stackOverflow.query_to_pandas_safe(query3, max_gb_scanned=30)\nresponse3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_lines_in_snippets_df = response3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalized_mean_df=(new_lines_in_snippets_df-new_lines_in_snippets_df.mean())/new_lines_in_snippets_df.std()\nplt.hist(new_lines_in_snippets_df['f0_'], bins=range(3, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_snippets = len(new_lines_in_snippets_df.index)\nprint('Total snippets: %d' % total_snippets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter out all code snippets tahat are less than four lines loong using the elbow method. First try to get all in one dataset. If that fails, try to load batch."},{"metadata":{"trusted":true},"cell_type":"code","source":"#query4 = 'SELECT answer_id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" OR question.tags LIKE \"%android%\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4\n#query4 = 'SELECT answer_id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags = \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 3'\n\nquery4 = '''\nSELECT answer_id as id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags = \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4\n\nUNION ALL\n\n(SELECT question_id as id, unnested_code_snippets FROM \n    (SELECT question_id, REGEXP_EXTRACT_ALL(question_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet \n    FROM\n    (SELECT question.id as question_id, question.body as question_body \n    FROM bigquery-public-data.stackoverflow.posts_questions as question\n    WHERE question.tags = \"java\" AND question.body LIKE \"%<code>%\")), \n    UNNEST(code_snippet) unnested_code_snippets \n    WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4)\n'''\n\ncode_snippets = stackOverflow.query_to_pandas_safe(query4, max_gb_scanned=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets_num = len(code_snippets.index)\nprint('Number of snippets: %d\\nNumber of 4+ snippets: %d\\nReduction: %f' % (total_snippets, code_snippets_num, code_snippets_num / total_snippets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets['unnested_code_snippets'].str.len().plot.hist(bins=500, legend=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets['unnested_code_snippets'].str.len().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor s in code_snippets['unnested_code_snippets']:\n    if s.count('\\n') == 15 and i < 10:\n        print(s)\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets.to_csv('java_questions_and_answers_4+_snippets.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"java_tokens = '''\nabstract\nassert\nboolean\nbreak\nbyte\ncase\ncatch\nchar\nclass\ncontinue\ndefault\ndo\ndouble\nelse\nenum\nextends\nfinal\nfinally\nfloat\nfor\nif\nimplements\nimport\ninstanceof\nint\ninterface\nlong\nnative\nnew\npackage\nprivate\nprotected\npublic\nreturn\nshort\nstatic\nstrictfp\nsuper\nswitch\nsynchronized\nthis\nthrow\nthrows\ntransient\ntry\nvoid\nvolatile\nwhile\n'''\njava_tokens = java_tokens.split('\\n')[1:-1]\njava_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data obtained"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ncode_snippets = pd.read_csv('./java_questions_and_answers_4+_snippets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets['unnested_code_snippets'].str.len().plot.hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(code_snippets.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'|'.join(java_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = code_snippets[code_snippets['unnested_code_snippets'].str.contains('|'.join(java_tokens))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.shape[0] / code_snippets.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets = new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from html.parser import HTMLParser\nimport html\n\nstart_tag = '<code>'\nend_tag = '</code>'\n\nparser = HTMLParser()\nraw_snippet = code_snippets.at[0, 'unnested_code_snippets'][len(start_tag):-len(end_tag)]\nprint(raw_snippet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded = html.unescape(raw_snippet)\nprint(decoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create blob\n\nsnip1, snip2, snip3, snip4, snip5, snip6....\n\n'\\n'.join(html.unescape(snip) for snip in all_snippets)\n\nsnip1Xsnip2Xsnip3Xsnip4X....\n\nlen(snip1)\nlen(snip1) + len(snip2) + 1\nlabel(n) = label(n - 1) + label(n) + 1\n\ndividing_newlines = dict()\nlen(snip1)\nlen(snip1) + len(snip2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"snippets_list = code_snippets['unnested_code_snippets'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def force_ascii(string):\n    encoded_string = string.encode('ascii', 'ignore')\n    return encoded_string.decode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_snippets = [force_ascii(html.unescape(raw_snippet[len(start_tag):-len(end_tag)]).strip('\\n \\t\\r')) for raw_snippet in snippets_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100, 500):\n    print(clean_snippets[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"giant_block = '\\n'.join(clean_snippets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_snippets.shape[0] / len(giant_block)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segment_new_lines = [len(clean_snippets[0])]\nfor i in range(1, len(clean_snippets)):\n    segment_new_lines.append(segment_new_lines[i - 1] +  len(clean_snippets[i]) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"giant_block[segment_new_lines[123]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install algorithms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfrom tqdm import tqdm\nimport gc\nimport os\nimport numpy as np\n\ndef datapoints(code_block, window_size, dividing_newlines):\n    i = 0\n    label_container = []\n    window_container = []\n    total = 0 # samples\n    \n    for window_start in tqdm(range(0, len(code_block), window_size)):\n        labels = []\n        window = []\n        \n        for ch_index in range(window_start, min(window_start+window_size, len(code_block))):\n            if dividing_newlines[i] != ch_index:\n                labels.append(0)\n            else:\n                labels.append(1)\n                i += 1\n            window.append(ord(code_block[ch_index]))\n\n        if len(window) == window_size:\n          label_container.append(np.array(labels, dtype='int'))\n          window_container.append(np.array(window, dtype='float64'))\n          total += 1\n\n        #window_np = [ord(ch) for ch in code_block[window_start:window_start+window_size]]\n        #labels_np = labels\n        \n        #yield (window_np, labels_np)\n        #yield labels\n    \n    return window_container, label_container\n\nwindows, labels = datapoints(giant_block, 100, segment_new_lines)\nwindows = np.array(windows, dtype='float64')\nnp.save('java_questions_and_answers_4+_windows.npy', windows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del windows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(labels, dtype='int')\nnp.save('java_labels.npy', labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from algorithms.search import binary_search\nimport numpy as np\nimport h5py\nimport pickle\nfrom tqdm import tqdm\nimport gc\nimport os\n\nCHUNK_CAPACITY = 2**17\n\n''' return (window, labels) '''\ndef datapoints(code_block, window_size, dividing_newlines):\n    i = 0\n    label_container = []\n    window_container = []\n    chunk = 0\n    total = 0\n    \n    if not os.path.exists('tmp'):\n        os.mkdir('tmp')\n    \n    for window_start in tqdm(range(0, len(code_block), window_size)):\n        labels = []\n        window = []\n        \n        for ch_index in range(window_start, min(window_start+window_size, len(code_block))):\n            if dividing_newlines[i] != ch_index:\n                labels.append(0)\n            else:\n                labels.append(1)\n                i += 1\n            window.append(ord(code_block[ch_index]))\n        \n        label_container.append(np.array(labels))\n        window_container.append(np.array(window))\n        total += 1\n        \n        if len(window_container) == CHUNK_CAPACITY:\n            chunkd = {'windows': window_container, 'labels': label_container}\n            with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:\n                pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            chunk += 1\n            label_container = []\n            window_container = []\n    \n    # cleaning\n    if len(window_container) > 0:\n        chunkd = {'windows': window_container, 'labels': label_container}\n        with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:\n            pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        chunk += 1\n        del chunkd\n        label_container = []\n        window_container = []\n                \n        #window_np = [ord(ch) for ch in code_block[window_start:window_start+window_size]]\n        #labels_np = labels\n        \n        #yield (window_np, labels_np)\n        #yield labels\n    \n    return total, chunk\n\ntotal, chunks = datapoints(giant_block, 100, segment_new_lines)\n#points = 0\n#for datapoint in datapoints(giant_block, 100, segment_new_lines):\n#    points += 1\n#labels, frame = datapoints(giant_block, 100, segment_new_lines)\n\n#window, labels = next(g)\n#len(window) == len(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine chunk load time"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nwatch_point = time.time()\n\nwith open('tmp/x_chunk_0.pickle', 'rb') as handle:\n    b = pickle.load(handle)\n\nprint('[' + str(CHUNK_CAPACITY) + '] load time: ' + str(time.time() - watch_point))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try without last\ntotal_new = int(total / CHUNK_CAPACITY) * CHUNK_CAPACITY\nchunks_new = int(total / CHUNK_CAPACITY)\nprint(total_new / total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Train/Validation/Test datasets"},{"metadata":{},"cell_type":"markdown","source":"......................... giant block .............................\n                   train                     |  valid     |  test\n                   \ntrain_size = int(len(giant_block) * .8); test_size = int(len(giant_block) * .1); valid_size = int(len(giant_block) * .1)\n\n\ntrain_start = 0; train_end = train_size\n\n\nvalid_start = train_end; valid_end = valid_start + valid_size\n\n\ntest_start = valid_end; test_end = test_start + test_size"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import Sequence\nimport random\n\nclass DataGenerator(Sequence):\n    def __init__(self, giant_block, index_start, index_end, window_size, dividing_newlines, batch_size):\n        self.giant_block = giant_block\n        self.index_start = index_start\n        self.index_end = index_end\n        self.window_size = window_size\n        self.dividing_newlines = dividing_newlines\n        self.batch_size = batch_size\n        \n        self._indices = list(range(0, len(self)))\n        self.on_epoch_end()\n        \n        \n    def __len__(self):\n        samples = (self.index_end - self.index_start) / self.window_size\n        return int(np.floor(samples / self.batch_size))\n    \n    \n    def __getitem__(self, index):\n        mapped_index = self._indices[index] # make it random\n        batch_index_start = self.index_start + mapped_index * self.window_size\n        \n        labels = []\n        \n        for ch_index in range(batch_index_start, batch_index_start + self.window_size):\n            index = binary_search(self.dividing_newlines, ch_index)\n            \n            if index is None:\n                labels.append(0)\n            else:\n                labels.append(1)\n                \n        window = [ord(ch) for ch in self.giant_block[batch_index_start: batch_index_start + self.window_size]]\n        \n        return np.array(window), np.array(labels)\n    \n        \n    def on_epoch_end(self):\n        random.shuffle(self._indices)\n    \n#ds = DataGenerator(giant_block, 0, int(len(giant_block) * .8), 100, segment_new_lines, 128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"class GDataGenerator(Sequence):\n    def __init__(self, chunks_names, batch_size):\n        self.total_chunks = len(chunks_names)\n        self.total_samples = self.total_chunks * CHUNK_CAPACITY\n        self.batch_size = batch_size\n        self.chunks_names = chunks_names\n        \n        self.chunk_indices = list(range(self.total_chunks)) # take care of last\n        \n        self.loaded_chunks = 0\n        self.loaded_chunk = None\n        \n        self.on_epoch_end()\n        \n    # FOR NOW PROVIDE FULL\n    def __len__(self):\n        return int(self.total_samples / self.batch_size)\n    \n    def __getitem__(self, index):\n        if not (index < self.loaded_chunks * CHUNK_CAPACITY):\n            with open('tmp/x_chunk_%d.pickle' % (self.chunks_names[self.loaded_chunks]), 'rb') as handle:\n                self.loaded_chunk = pickle.load(handle)\n                self.loaded_chunks += 1\n        \n        return self.loaded_chunk['windows'][index % CHUNK_CAPACITY], self.loaded_chunk['labels'][index % CHUNK_CAPACITY]\n    \n    def on_epoch_end(self):\n        self.chunk_indices = list(range(self.total_chunks - 1))\n        random.shuffle(self.chunk_indices)\n        self.chunk_indices.append(self.total_chunks - 1) # because last is not full\n\nd = GDataGenerator(range(chunks_new), 128)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import Sequence\nimport random\n\nclass GDataGenerator(Sequence):\n    def __init__(self, chunks_names, batch_size):\n        self.total_chunks = len(chunks_names)\n        self.total_samples = self.total_chunks * CHUNK_CAPACITY\n        self.batch_size = batch_size\n        self.chunks_names = chunks_names\n        \n        self.chunk_indices = list(range(self.total_chunks)) # take care of last\n        \n        self.loaded_chunks = 0\n        self.loaded_chunk = None\n        \n        self.on_epoch_end()\n        \n    # FOR NOW PROVIDE FULL\n    def __len__(self):\n        return int(self.total_samples / self.batch_size)\n    \n    def __getitem__(self, index):\n        if not (index < self.loaded_chunks * CHUNK_CAPACITY):\n            with open('tmp/x_chunk_%d.pickle' % (self.chunks_names[self.loaded_chunks]), 'rb') as handle:\n                self.loaded_chunk = pickle.load(handle)\n                self.loaded_chunks += 1\n\n        #X = np.empty((128, 100))\n\n        X = np.array(self.loaded_chunk['windows'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='float64')\n        y = np.array(self.loaded_chunk['labels'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='float64')\n        \n        return X, y\n    \n\n    def on_epoch_end(self):\n        print('shuffle')\n        self.chunk_indices = list(range(self.total_chunks - 1))\n        random.shuffle(self.chunk_indices)\n        self.chunk_indices.append(self.total_chunks - 1) # because last is not full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Procentage of true positive\n100 * len(segment_new_lines) / len(giant_block)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(len(giant_block) * .8)\ntest_size = int(len(giant_block) * .1)\nvalid_size = int(len(giant_block) * .1)\n\ntrain_start = 0\ntrain_end = train_size\n\nvalid_start = train_end\nvalid_end = valid_start + valid_size\n\ntest_start = valid_end\ntest_end = test_start + test_size\n\n\n# gen\nall_chunks = list(range(chunks_new))\n\ngtrain_lim = int(len(all_chunks) * .8)\ngvalid_lim = gtrain_lim + int(len(all_chunks) * .1)\n\ngtrain = all_chunks[:gtrain_lim]\ngvalid = all_chunks[gtrain_lim:gvalid_lim]\ngtest = all_chunks[gvalid_lim:]\nlen(gtrain)/len(all_chunks), len(gvalid)/len(all_chunks), len(gtest) / len(all_chunks)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"'''\ndata_points_x = []\ndata_points_y = []\ni = 0\nfor dp in datapoints(giant_block, 100, segment_new_lines):\n    data_points_x.append(dp[0])\n    data_points_y.append(dp[1])\n'''\n#training_generator = DataGenerator(giant_block, train_start, train_end, 100, segment_new_lines, 128)\n#validation_generator = DataGenerator(giant_block, valid_start, valid_end, 100, segment_new_lines, 128)\n\ngtraining_generator = GDataGenerator(gtrain, 128)\ngvalidation_generator = GDataGenerator(gvalid, 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef positive_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    return true_positives\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense, TimeDistributed, Flatten\nfrom keras.metrics import AUC\n\nn_vocab = 256 \nnum_lstm_units = 256 \nbimodel = Sequential() \nbimodel.add(Embedding(n_vocab,20, input_length =100)) \nbimodel.add(Dropout(.2)) \nbimodel.add(Bidirectional(LSTM(num_lstm_units, return_sequences=True))) \nbimodel.add(Dropout(.2)) \nbimodel.add(TimeDistributed(Dense(150, activation='relu'))) \nbimodel.add(Dropout(.2)) \nbimodel.add(TimeDistributed(Dense(75, activation = 'relu'))) \nbimodel.add(Dropout(.2)) \nbimodel.add(TimeDistributed(Dense(1, activation = 'sigmoid'))) \nbimodel.add(Flatten())\nbimodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['acc', AUC(curve='PR')]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> model = bimodel.fit(\n>     train_x,\n>     train_y,\n>     batch_size=128,\n>     epochs=20,\n>     validation_data=(val_x, val_y))"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_filepath = 'model-{epoch}.h5'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    period=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = bimodel.fit_generator(\n    generator=gtraining_generator,\n    validation_data=gvalidation_generator,\n    use_multiprocessing=True,\n    workers=4,\n    callbacks=[model_checkpoint_callback],\n    epochs=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}