{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import bq_helper\n",
    "from bq_helper import BigQueryHelper\n",
    "\n",
    "stackOverflow = BigQueryHelper(active_project='bigquery-public-data', dataset_name='stackoverflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"stackoverflow\")\n",
    "bq_assistant.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = 'SELECT x.id, x.body FROM bigquery-public-data.stackoverflow.posts_questions as x WHERE x.tags LIKE \"%java%\" OR x.tags LIKE \"%android%\" AND x.body LIKE \"%<code>%\"'\n",
    "#query = 'SELECT COUNT(x) FROM bigquery-public-data.stackoverflow.posts_answers as x WHERE x.body LIKE \"%<code>%\"'\n",
    "#response = stackOverflow.query_to_pandas_safe(query, max_gb_scanned=30)\n",
    "#response\n",
    "\n",
    "#query = 'SELECT ide, REGEXP_EXTRACT_ALL(body, r\\'(<code>[\\s\\S]*?<\\/code>)\\') as code FROM (SELECT x.id as ide, x.body as body FROM bigquery-public-data.stackoverflow.posts_questions as x WHERE x.tags LIKE \"%java%\" OR x.tags LIKE \"%android%\" AND x.body LIKE \"%<code>%\")'\n",
    "#response2 = stackOverflow.query_to_pandas_safe(query, max_gb_scanned=30)\n",
    "#response2\n",
    "\n",
    "#query3 = 'SELECT answer_id, LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\")) FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" OR question.tags LIKE \"%android%\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets'\n",
    "query3 = 'SELECT answer_id, LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\")) FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets'\n",
    "\n",
    "response3 = stackOverflow.query_to_pandas_safe(query3, max_gb_scanned=30)\n",
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines_in_snippets_df = response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_mean_df=(new_lines_in_snippets_df-new_lines_in_snippets_df.mean())/new_lines_in_snippets_df.std()\n",
    "plt.hist(new_lines_in_snippets_df['f0_'], bins=range(3, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_snippets = len(new_lines_in_snippets_df.index)\n",
    "print('Total snippets: %d' % total_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all code snippets tahat are less than four lines loong using the elbow method. First try to get all in one dataset. If that fails, try to load batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query4 = 'SELECT answer_id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags LIKE \"java\" OR question.tags LIKE \"%android%\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4\n",
    "#query4 = 'SELECT answer_id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags = \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 3'\n",
    "\n",
    "query4 = '''\n",
    "SELECT answer_id as id, unnested_code_snippets FROM (SELECT answer_id, REGEXP_EXTRACT_ALL(answer_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet FROM(SELECT answer.id as answer_id,answer.body as answer_body FROM bigquery-public-data.stackoverflow.posts_answers as answer JOIN bigquery-public-data.stackoverflow.posts_questions as question ON answer.parent_id = question.id WHERE question.tags = \"java\" AND answer.body LIKE \"%<code>%\")), UNNEST(code_snippet) unnested_code_snippets WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "(SELECT question_id as id, unnested_code_snippets FROM \n",
    "    (SELECT question_id, REGEXP_EXTRACT_ALL(question_body, r\"(<code>[\\s\\S]*?<\\/code>)\") as code_snippet \n",
    "    FROM\n",
    "    (SELECT question.id as question_id, question.body as question_body \n",
    "    FROM bigquery-public-data.stackoverflow.posts_questions as question\n",
    "    WHERE question.tags = \"java\" AND question.body LIKE \"%<code>%\")), \n",
    "    UNNEST(code_snippet) unnested_code_snippets \n",
    "    WHERE (LENGTH(unnested_code_snippets) - LENGTH(REGEXP_REPLACE(unnested_code_snippets, \"\\\\n\", \"\"))) >= 4)\n",
    "'''\n",
    "\n",
    "code_snippets = stackOverflow.query_to_pandas_safe(query4, max_gb_scanned=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets_num = len(code_snippets.index)\n",
    "print('Number of snippets: %d\\nNumber of 4+ snippets: %d\\nReduction: %f' % (total_snippets, code_snippets_num, code_snippets_num / total_snippets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets['unnested_code_snippets'].str.len().plot.hist(bins=500, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets['unnested_code_snippets'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for s in code_snippets['unnested_code_snippets']:\n",
    "    if s.count('\\n') == 15 and i < 10:\n",
    "        print(s)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets.to_csv('java_questions_and_answers_4+_snippets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_tokens = '''\n",
    "abstract\n",
    "assert\n",
    "boolean\n",
    "break\n",
    "byte\n",
    "case\n",
    "catch\n",
    "char\n",
    "class\n",
    "continue\n",
    "default\n",
    "do\n",
    "double\n",
    "else\n",
    "enum\n",
    "extends\n",
    "final\n",
    "finally\n",
    "float\n",
    "for\n",
    "if\n",
    "implements\n",
    "import\n",
    "instanceof\n",
    "int\n",
    "interface\n",
    "long\n",
    "native\n",
    "new\n",
    "package\n",
    "private\n",
    "protected\n",
    "public\n",
    "return\n",
    "short\n",
    "static\n",
    "strictfp\n",
    "super\n",
    "switch\n",
    "synchronized\n",
    "this\n",
    "throw\n",
    "throws\n",
    "transient\n",
    "try\n",
    "void\n",
    "volatile\n",
    "while\n",
    "'''\n",
    "java_tokens = java_tokens.split('\\n')[1:-1]\n",
    "java_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "code_snippets = pd.read_csv('./java_questions_and_answers_4+_snippets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets['unnested_code_snippets'].str.len().plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_snippets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'|'.join(java_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = code_snippets[code_snippets['unnested_code_snippets'].str.contains('|'.join(java_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape[0] / code_snippets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import html\n",
    "\n",
    "start_tag = '<code>'\n",
    "end_tag = '</code>'\n",
    "\n",
    "parser = HTMLParser()\n",
    "raw_snippet = code_snippets.at[0, 'unnested_code_snippets'][len(start_tag):-len(end_tag)]\n",
    "print(raw_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = html.unescape(raw_snippet)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create blob\n",
    "\n",
    "snip1, snip2, snip3, snip4, snip5, snip6....\n",
    "\n",
    "'\\n'.join(html.unescape(snip) for snip in all_snippets)\n",
    "\n",
    "snip1Xsnip2Xsnip3Xsnip4X....\n",
    "\n",
    "len(snip1)\n",
    "len(snip1) + len(snip2) + 1\n",
    "label(n) = label(n - 1) + label(n) + 1\n",
    "\n",
    "dividing_newlines = dict()\n",
    "len(snip1)\n",
    "len(snip1) + len(snip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets_list = code_snippets['unnested_code_snippets'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_ascii(string):\n",
    "    encoded_string = string.encode('ascii', 'ignore')\n",
    "    return encoded_string.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_snippets = [force_ascii(html.unescape(raw_snippet[len(start_tag):-len(end_tag)]).strip('\\n \\t\\r')) for raw_snippet in snippets_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100, 500):\n",
    "    print(clean_snippets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_block = '\\n'.join(clean_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippets.shape[0] / len(giant_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_new_lines = [len(clean_snippets[0])]\n",
    "for i in range(1, len(clean_snippets)):\n",
    "    segment_new_lines.append(segment_new_lines[i - 1] +  len(clean_snippets[i]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_block[segment_new_lines[123]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def datapoints(code_block, window_size, dividing_newlines):\n",
    "    i = 0\n",
    "    label_container = []\n",
    "    window_container = []\n",
    "    total = 0 # samples\n",
    "    \n",
    "    for window_start in tqdm(range(0, len(code_block), window_size)):\n",
    "        labels = []\n",
    "        window = []\n",
    "        \n",
    "        for ch_index in range(window_start, min(window_start+window_size, len(code_block))):\n",
    "            if dividing_newlines[i] != ch_index:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                i += 1\n",
    "            window.append(ord(code_block[ch_index]))\n",
    "\n",
    "        if len(window) == window_size:\n",
    "          label_container.append(np.array(labels, dtype='int'))\n",
    "          window_container.append(np.array(window, dtype='float64'))\n",
    "          total += 1\n",
    "\n",
    "        #window_np = [ord(ch) for ch in code_block[window_start:window_start+window_size]]\n",
    "        #labels_np = labels\n",
    "        \n",
    "        #yield (window_np, labels_np)\n",
    "        #yield labels\n",
    "    \n",
    "    return window_container, label_container\n",
    "\n",
    "windows, labels = datapoints(giant_block, 100, segment_new_lines)\n",
    "windows = np.array(windows, dtype='float64')\n",
    "np.save('java_questions_and_answers_4+_windows.npy', windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype='int')\n",
    "np.save('java_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.search import binary_search\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "CHUNK_CAPACITY = 2**17\n",
    "\n",
    "''' return (window, labels) '''\n",
    "def datapoints(code_block, window_size, dividing_newlines):\n",
    "    i = 0\n",
    "    label_container = []\n",
    "    window_container = []\n",
    "    chunk = 0\n",
    "    total = 0\n",
    "    \n",
    "    if not os.path.exists('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "    \n",
    "    for window_start in tqdm(range(0, len(code_block), window_size)):\n",
    "        labels = []\n",
    "        window = []\n",
    "        \n",
    "        for ch_index in range(window_start, min(window_start+window_size, len(code_block))):\n",
    "            if dividing_newlines[i] != ch_index:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                i += 1\n",
    "            window.append(ord(code_block[ch_index]))\n",
    "        \n",
    "        label_container.append(np.array(labels))\n",
    "        window_container.append(np.array(window))\n",
    "        total += 1\n",
    "        \n",
    "        if len(window_container) == CHUNK_CAPACITY:\n",
    "            chunkd = {'windows': window_container, 'labels': label_container}\n",
    "            with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:\n",
    "                pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            chunk += 1\n",
    "            label_container = []\n",
    "            window_container = []\n",
    "    \n",
    "    # cleaning\n",
    "    if len(window_container) > 0:\n",
    "        chunkd = {'windows': window_container, 'labels': label_container}\n",
    "        with open('tmp/x_chunk_%d.pickle' % (chunk,), 'wb') as handle:\n",
    "            pickle.dump(chunkd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        chunk += 1\n",
    "        del chunkd\n",
    "        label_container = []\n",
    "        window_container = []\n",
    "                \n",
    "        #window_np = [ord(ch) for ch in code_block[window_start:window_start+window_size]]\n",
    "        #labels_np = labels\n",
    "        \n",
    "        #yield (window_np, labels_np)\n",
    "        #yield labels\n",
    "    \n",
    "    return total, chunk\n",
    "\n",
    "total, chunks = datapoints(giant_block, 100, segment_new_lines)\n",
    "#points = 0\n",
    "#for datapoint in datapoints(giant_block, 100, segment_new_lines):\n",
    "#    points += 1\n",
    "#labels, frame = datapoints(giant_block, 100, segment_new_lines)\n",
    "\n",
    "#window, labels = next(g)\n",
    "#len(window) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine chunk load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "watch_point = time.time()\n",
    "\n",
    "with open('tmp/x_chunk_0.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print('[' + str(CHUNK_CAPACITY) + '] load time: ' + str(time.time() - watch_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without last\n",
    "total_new = int(total / CHUNK_CAPACITY) * CHUNK_CAPACITY\n",
    "chunks_new = int(total / CHUNK_CAPACITY)\n",
    "print(total_new / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Validation/Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "......................... giant block .............................\n",
    "                   train                     |  valid     |  test\n",
    "                   \n",
    "train_size = int(len(giant_block) * .8); test_size = int(len(giant_block) * .1); valid_size = int(len(giant_block) * .1)\n",
    "\n",
    "\n",
    "train_start = 0; train_end = train_size\n",
    "\n",
    "\n",
    "valid_start = train_end; valid_end = valid_start + valid_size\n",
    "\n",
    "\n",
    "test_start = valid_end; test_end = test_start + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, giant_block, index_start, index_end, window_size, dividing_newlines, batch_size):\n",
    "        self.giant_block = giant_block\n",
    "        self.index_start = index_start\n",
    "        self.index_end = index_end\n",
    "        self.window_size = window_size\n",
    "        self.dividing_newlines = dividing_newlines\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self._indices = list(range(0, len(self)))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        samples = (self.index_end - self.index_start) / self.window_size\n",
    "        return int(np.floor(samples / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        mapped_index = self._indices[index] # make it random\n",
    "        batch_index_start = self.index_start + mapped_index * self.window_size\n",
    "        \n",
    "        labels = []\n",
    "        \n",
    "        for ch_index in range(batch_index_start, batch_index_start + self.window_size):\n",
    "            index = binary_search(self.dividing_newlines, ch_index)\n",
    "            \n",
    "            if index is None:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                \n",
    "        window = [ord(ch) for ch in self.giant_block[batch_index_start: batch_index_start + self.window_size]]\n",
    "        \n",
    "        return np.array(window), np.array(labels)\n",
    "    \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self._indices)\n",
    "    \n",
    "#ds = DataGenerator(giant_block, 0, int(len(giant_block) * .8), 100, segment_new_lines, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class GDataGenerator(Sequence):\n",
    "    def __init__(self, chunks_names, batch_size):\n",
    "        self.total_chunks = len(chunks_names)\n",
    "        self.total_samples = self.total_chunks * CHUNK_CAPACITY\n",
    "        self.batch_size = batch_size\n",
    "        self.chunks_names = chunks_names\n",
    "        \n",
    "        self.chunk_indices = list(range(self.total_chunks)) # take care of last\n",
    "        \n",
    "        self.loaded_chunks = 0\n",
    "        self.loaded_chunk = None\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # FOR NOW PROVIDE FULL\n",
    "    def __len__(self):\n",
    "        return int(self.total_samples / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if not (index < self.loaded_chunks * CHUNK_CAPACITY):\n",
    "            with open('tmp/x_chunk_%d.pickle' % (self.chunks_names[self.loaded_chunks]), 'rb') as handle:\n",
    "                self.loaded_chunk = pickle.load(handle)\n",
    "                self.loaded_chunks += 1\n",
    "        \n",
    "        return self.loaded_chunk['windows'][index % CHUNK_CAPACITY], self.loaded_chunk['labels'][index % CHUNK_CAPACITY]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.chunk_indices = list(range(self.total_chunks - 1))\n",
    "        random.shuffle(self.chunk_indices)\n",
    "        self.chunk_indices.append(self.total_chunks - 1) # because last is not full\n",
    "\n",
    "d = GDataGenerator(range(chunks_new), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "class GDataGenerator(Sequence):\n",
    "    def __init__(self, chunks_names, batch_size):\n",
    "        self.total_chunks = len(chunks_names)\n",
    "        self.total_samples = self.total_chunks * CHUNK_CAPACITY\n",
    "        self.batch_size = batch_size\n",
    "        self.chunks_names = chunks_names\n",
    "        \n",
    "        self.chunk_indices = list(range(self.total_chunks)) # take care of last\n",
    "        \n",
    "        self.loaded_chunks = 0\n",
    "        self.loaded_chunk = None\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # FOR NOW PROVIDE FULL\n",
    "    def __len__(self):\n",
    "        return int(self.total_samples / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if not (index < self.loaded_chunks * CHUNK_CAPACITY):\n",
    "            with open('tmp/x_chunk_%d.pickle' % (self.chunks_names[self.loaded_chunks]), 'rb') as handle:\n",
    "                self.loaded_chunk = pickle.load(handle)\n",
    "                self.loaded_chunks += 1\n",
    "\n",
    "        #X = np.empty((128, 100))\n",
    "\n",
    "        X = np.array(self.loaded_chunk['windows'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='float64')\n",
    "        y = np.array(self.loaded_chunk['labels'][index % CHUNK_CAPACITY: index % CHUNK_CAPACITY + self.batch_size], dtype='float64')\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        print('shuffle')\n",
    "        self.chunk_indices = list(range(self.total_chunks - 1))\n",
    "        random.shuffle(self.chunk_indices)\n",
    "        self.chunk_indices.append(self.total_chunks - 1) # because last is not full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procentage of true positive\n",
    "100 * len(segment_new_lines) / len(giant_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(giant_block) * .8)\n",
    "test_size = int(len(giant_block) * .1)\n",
    "valid_size = int(len(giant_block) * .1)\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_size\n",
    "\n",
    "valid_start = train_end\n",
    "valid_end = valid_start + valid_size\n",
    "\n",
    "test_start = valid_end\n",
    "test_end = test_start + test_size\n",
    "\n",
    "\n",
    "# gen\n",
    "all_chunks = list(range(chunks_new))\n",
    "\n",
    "gtrain_lim = int(len(all_chunks) * .8)\n",
    "gvalid_lim = gtrain_lim + int(len(all_chunks) * .1)\n",
    "\n",
    "gtrain = all_chunks[:gtrain_lim]\n",
    "gvalid = all_chunks[gtrain_lim:gvalid_lim]\n",
    "gtest = all_chunks[gvalid_lim:]\n",
    "len(gtrain)/len(all_chunks), len(gvalid)/len(all_chunks), len(gtest) / len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "data_points_x = []\n",
    "data_points_y = []\n",
    "i = 0\n",
    "for dp in datapoints(giant_block, 100, segment_new_lines):\n",
    "    data_points_x.append(dp[0])\n",
    "    data_points_y.append(dp[1])\n",
    "'''\n",
    "#training_generator = DataGenerator(giant_block, train_start, train_end, 100, segment_new_lines, 128)\n",
    "#validation_generator = DataGenerator(giant_block, valid_start, valid_end, 100, segment_new_lines, 128)\n",
    "\n",
    "gtraining_generator = GDataGenerator(gtrain, 128)\n",
    "gvalidation_generator = GDataGenerator(gvalid, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def positive_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    return true_positives\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense, TimeDistributed, Flatten\n",
    "from keras.metrics import AUC\n",
    "\n",
    "n_vocab = 256 \n",
    "num_lstm_units = 256 \n",
    "bimodel = Sequential() \n",
    "bimodel.add(Embedding(n_vocab,20, input_length =100)) \n",
    "bimodel.add(Dropout(.2)) \n",
    "bimodel.add(Bidirectional(LSTM(num_lstm_units, return_sequences=True))) \n",
    "bimodel.add(Dropout(.2)) \n",
    "bimodel.add(TimeDistributed(Dense(150, activation='relu'))) \n",
    "bimodel.add(Dropout(.2)) \n",
    "bimodel.add(TimeDistributed(Dense(75, activation = 'relu'))) \n",
    "bimodel.add(Dropout(.2)) \n",
    "bimodel.add(TimeDistributed(Dense(1, activation = 'sigmoid'))) \n",
    "bimodel.add(Flatten())\n",
    "bimodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['acc', AUC(curve='PR')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model = bimodel.fit(\n",
    ">     train_x,\n",
    ">     train_y,\n",
    ">     batch_size=128,\n",
    ">     epochs=20,\n",
    ">     validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_filepath = 'model-{epoch}.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    period=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bimodel.fit_generator(\n",
    "    generator=gtraining_generator,\n",
    "    validation_data=gvalidation_generator,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    callbacks=[model_checkpoint_callback],\n",
    "    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
